{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
    "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset. \n",
    "   \n",
    "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\" \n",
    "\n",
    "Q1. Describe each of those datasets and their source in 100-200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "The selected dataset for this assignment comprises two classic literary works from Project Gutenberg: Shakespeare's \"Romeo and Juliet\" and a collection of Brazilian Tales by various authors, including pieces from the renowned Brazilian novelist Machado de Assis (one of my favorite Brazilian authors). I chose these two texts due to their allegedly rich linguistic content, and both significantly exceeded the minimum requirement of 10,000 tokens for analysis. Moreover, I also wanted to analyze a text originally written in a different language, considering that the translation process might impact the vocabulary richness and complexity of the book.\n",
    "\n",
    "However, in line with the class objectives of exploring and working with personal data, two additional datasets were included. One dataset is my own Statement of Purpose, used for my admission to the MIMS program here at Cal. The second dataset consists of an essay generated by Chat GPT, leveraging the GPT-3.5 model, with a similar goal of crafting a Statement of Purpose for myself.\n",
    "\n",
    "The diversity of these datasets, from classic literature to personal narratives and AI-generated text, promises a multifaceted exploration of natural language processing techniques and the opportunity to gain insights into the distinctive linguistic features that characterize each text category.\n",
    "\n",
    "**Additional Information:** For the ChatGPT-generated essay, I used the prompt: \"Write a Statement of Purpose for Filipe Santos' application to a Master's program in Information Management and Systems at the University of California Berkeley. The Statement of Purpose should describe Filipe's relevant academic and professional experience and accomplishments, his future professional goals once the degree is acquired, and why he is drawn to the MIMS program and believes it would be a good fit for him and his goals. Filipe is a Brazilian industrial engineer with extensive experience in the tech world, being part of an early-stage startup from the beginning until its successful M&A. Filipe is also very interested in Smart Cities and how relevant education is as a transformational factor. The essay should be 2-3 pages long.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "#1. From the ExploreTokenization Jupyter Notebook, it firstly sets up some basic parameters for the nlp and instantiate the nlp tokenizer.\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['tagger,ner,parser'])\n",
    "    nlp.remove_pipe('ner')\n",
    "    nlp.remove_pipe('parser');\n",
    "    tokenizer = nlp.tokenizer\n",
    "    spacy_tokens = []\n",
    "\n",
    "#2. Then it generates a list called spacy_tokens, that stores all the tokenized records of the text. It also references the code from the ExploreTokenization notebook.\n",
    "    with open(filename, encoding=\"utf-8-sig\") as file:\n",
    "        text = file.read().lower()\n",
    "        for line in text.splitlines():\n",
    "            spacy_tokens.extend([token.text for token in nlp(line)])\n",
    "    return spacy_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these file paths to wherever the datasets you created above live.\n",
    "class1_tokens=read_and_tokenize(\"/Users/filipesantos/Documents/ANLP/class1_dataset.txt\")\n",
    "class2_tokens=read_and_tokenize(\"/Users/filipesantos/Documents/ANLP/class2_dataset.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As previously explained in this notebook, I included 4 different datasets for analysis, which are:\n",
    "#class1_dataset: Romeo and Juliet (Shakespeare)\n",
    "#class2_dataset: Brazilian_Tales (Multiple authors)\n",
    "#class3_dataset: UC Berkeley Statement of Purpose (Filipe Santos)\n",
    "#class4_dataset: LLM-generated Statement of Purpose (ChatGPT)\n",
    "\n",
    "class1_dataset = [\"/Users/filipesantos/Documents/ANLP/class1_dataset.txt\"]\n",
    "class2_dataset = [\"/Users/filipesantos/Documents/ANLP/class2_dataset.txt\"]\n",
    "class3_dataset = [\"/Users/filipesantos/Documents/ANLP/class3_dataset.txt\"]\n",
    "class4_dataset = [\"/Users/filipesantos/Documents/ANLP/class4_dataset.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we start, I added some specific changes to the previous version of the function because I wanted to adapt it to run not only for the two class_datasets that were part of the homework, but also to a different dataset. Therefore, the one_tokens and the two_tokens parameters should now refer to which of the classn_dataset the program should analyze. The suggested comparisons are either class1_dataset verus class2_dataset; or class3_dataset versus class4_dataset.\n",
    "\n",
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "\n",
    "    #1. It creates Counters for each class to calculate word frequencies\n",
    "    class1_word_counts = Counter()\n",
    "    class2_word_counts = Counter()\n",
    "\n",
    "    #2. It counts the word occurrences in both documents using the read_and_tokenize() function\n",
    "    for doc in one_tokens:\n",
    "        doc_tokens = read_and_tokenize(doc)  \n",
    "        class1_word_counts.update(doc_tokens)\n",
    "\n",
    "    for doc in two_tokens:\n",
    "        doc_tokens = read_and_tokenize(doc) \n",
    "        class2_word_counts.update(doc_tokens)\n",
    "\n",
    "    total_words_class1 = sum(class1_word_counts.values())\n",
    "    total_words_class2 = sum(class2_word_counts.values())\n",
    "\n",
    "    #3.  It calculates the sum of word frequencies across the two documents and the global parameter alpha_0, required in the formula provided on Q3.\n",
    "    all_word_counts = class1_word_counts + class2_word_counts\n",
    "    alpha_0 = len(all_word_counts) * 0.01\n",
    "\n",
    "    #4. It calculates the frequency of each word through a for-loop\n",
    "    log_odds_ratios = {}\n",
    "    for word, freq in all_word_counts.items():\n",
    "        class1_freq = class1_word_counts[word]\n",
    "        class2_freq = class2_word_counts[word]\n",
    "\n",
    "    #5. And then it applies the formula presented on Q3, breaking it down on the calculation of the numerator (log_odds_num) and the denominator (log_odds_denom.\n",
    "        log_odds_num = np.log((class1_freq + 0.01) / (total_words_class1 + 100 - 0.01 + alpha_0)) - np.log((class2_freq + 0.01) / (total_words_class2 + 100-0.01 + alpha_0))\n",
    "        log_odds_denom = np.sqrt((1/(class1_freq + 0.01))+(1/(class2_freq + 0.01)))\n",
    "        log_odds = log_odds_num / log_odds_denom\n",
    "        log_odds_ratios[word] = log_odds\n",
    "\n",
    "    #6. Finally, it sorts the words by the calculated log-odds ratio to find the top-n words for each class. N is defined by the 'display' parameter\n",
    "    top_class1_words = sorted(log_odds_ratios.items(), key=lambda x: x[1], reverse=True)[:display]\n",
    "    top_class2_words = sorted(log_odds_ratios.items(), key=lambda x: x[1], reverse=False)[:display]\n",
    "\n",
    "    print(\"Top 25 words for class1:\")\n",
    "    for word, log_odds in top_class1_words:\n",
    "        print(f\"{word}: {log_odds}\")\n",
    "\n",
    "    print(\"\\nTop 25 words for class2:\")\n",
    "    for word, log_odds in top_class2_words:\n",
    "        print(f\"{word}: {log_odds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words for class1:\n",
      ".: 16.02434997642278\n",
      "my: 10.143548785827782\n",
      "?: 9.848510769215055\n",
      "â€™s: 8.930977012975237\n",
      ",: 8.654346955643229\n",
      "love: 8.096973046887959\n",
      "i: 7.97171010408455\n",
      "]: 7.355755651275787\n",
      "o: 7.304737054147855\n",
      "[: 7.287653334126232\n",
      "thou: 6.799500851336238\n",
      "what: 6.495312899134617\n",
      "!: 6.456331322593055\n",
      "shall: 6.253815643879929\n",
      "me: 6.123529940683077\n",
      "lady: 6.0711931570506925\n",
      "good: 5.393093178220031\n",
      "night: 5.3455745228428855\n",
      "not: 5.306334470112611\n",
      "be: 5.173352483112935\n",
      "here: 5.0957844524344065\n",
      "death: 5.036230342517764\n",
      "thy: 5.019786257698284\n",
      "enter: 5.014015531533947\n",
      "come: 5.00489802592075\n",
      "\n",
      "Top 25 words for class2:\n",
      "the: -22.2518094643615\n",
      "of: -15.208750645695574\n",
      "was: -12.760877751558157\n",
      "he: -11.207616612977775\n",
      "his: -10.234678423358822\n",
      "had: -9.138179313738322\n",
      "at: -6.517570839844043\n",
      "in: -5.580092373575886\n",
      "who: -5.465441345423155\n",
      "into: -4.933633700813729\n",
      "little: -4.915903929780011\n",
      "only: -4.754835994048599\n",
      "were: -4.744128144995564\n",
      "has: -4.599517492157525\n",
      "(: -4.484133479632404\n",
      "upon: -4.452886265356546\n",
      "): -4.4508220201688715\n",
      "to: -4.3895224742096435\n",
      "been: -4.238170038888486\n",
      "its: -4.225338391458285\n",
      "life: -3.919930856069693\n",
      "came: -3.699656247587891\n",
      "other: -3.6328292787259784\n",
      "about: -3.482053111146591\n",
      "as: -3.429250900660545\n"
     ]
    }
   ],
   "source": [
    "logodds_with_uninformative_prior(class1_dataset, class2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
